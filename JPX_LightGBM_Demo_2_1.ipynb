{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8SxGyUXpdsV"
   },
   "source": [
    "# JPX LightGBM Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWK9Lk9bWDE9"
   },
   "source": [
    "## Read files \n",
    "* **stock_prices, options, financials, trades,   secondary_stock_price, stock_list**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKlUWA_Pph99"
   },
   "source": [
    "Contest overview\n",
    "* Japan Exchange Group (JPX) forecasts the rate of change in securities prices (closing prices) for 2000 securities\n",
    "* The objective variable is the rate of change in the closing price from the next day to the day after next.\n",
    "* The submitted data is not the value of the objective variable itself, but the order when the values ​​of the objective variable are arranged in descending order.\n",
    "* If you want to know more about the contest, [Japanese ver] Easy to understand the competition will be very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvHMX9EFqDHi"
   },
   "source": [
    "Notebook overview\n",
    "* In this notebook, **[Data reading]-> [Data integration]-> [Feature quantity engineering]-> [Learning]-> [Inference / evaluation]-> [Submission]** is performed all at once.\n",
    "* The model used is LightGBM.\n",
    "* Generate three models and ensemble the results to create the final inference result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l27d9xrcr3XA"
   },
   "source": [
    "The flow up to inference and evaluation is as follows --\n",
    "The red background is a function used only for inference / evaluation estimation. The blue background is the function / data used at the time of submission -- \n",
    "By customizing the function on the blue background, accuracy verification can be performed with various features, and submit can be performed as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJtcAUZZv6Cu"
   },
   "source": [
    "* **Read files**\n",
    "* stock_prices,options,financials,trades,secondary_stock_price,stock_list\n",
    "* **collector**\n",
    "* merge_data=> adjust price=> **base_df**\n",
    "* **preprocessor**\n",
    "* generate_features (add_columns_per_code=> add columns_per_day) => select_features => **feature_df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "66izqHRQ-mqu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# !unzip /content/drive/MyDrive/jpx-tokyo-stock-exchange-prediction.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y6PTAQVf4PLV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from decimal import ROUND_HALF_UP, Decimal\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoNfBrc7Pk0F"
   },
   "source": [
    "* **stock_list.csv**\n",
    "* **train_files （Date: 2017-01-04 ～ 2021-12-03）**\n",
    "* **supplemental_files （Date: 2021-12-06 ～ 2022-02-28 __ 2022/04/05(current)）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Lw1gbv6t4PI0"
   },
   "outputs": [],
   "source": [
    "def read_files(dir_name: str = 'data'):\n",
    "    base_path = Path(f'{dir_name}')\n",
    "    prices = pd.read_csv(base_path / 'stock_prices.csv')\n",
    "    options = pd.read_csv(base_path / 'options.csv')\n",
    "    financials = pd.read_csv(base_path / 'financials.csv')\n",
    "    trades = pd.read_csv(base_path / 'trades.csv')\n",
    "    secondary_prices = pd.read_csv(base_path / 'secondary_stock_prices.csv')\n",
    "    return prices, options, financials, trades, secondary_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aFGhKTL34PGr"
   },
   "outputs": [],
   "source": [
    "stock_list = pd.read_csv('data/stock_list.csv')\n",
    "train_files = read_files('data/train_files')\n",
    "supplemental_files = read_files('data/supplemental_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-B7Kv_5_RSKH"
   },
   "source": [
    "* The merge_data function merges each file horizontally. At the moment, we only use stock_prices and stock_list.\n",
    "* If you uncomment it, you can combine it with trades and financials,\n",
    "Since the timing at which valid records are generated is not daily for these data, it is necessary to take measures such as \"inheriting the values ​​of the most recently generated valid records\" in order to make them meaningful as learning data. increase.\n",
    "* Regarding options, if you look at the attached rule https://www.jpx.co.jp/derivatives/index.html) of OptionsCode, you can see the proper usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcdaViLoWOCK"
   },
   "source": [
    "## Collector ( merge_data  =>  adjust price  =>  **base_df** )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sphGBCM34PEV"
   },
   "outputs": [],
   "source": [
    "def merge_data(prices, options, financials, trades, secondary_prices, stock_list):\n",
    "    # Based on stock_prices\n",
    "    base_df = prices.copy()\n",
    "    \n",
    "    \n",
    "    # Combine with #stock_list\n",
    "    _stock_list = stock_list.copy()\n",
    "    _stock_list.rename(columns={'Close': 'Close_x'}, inplace=True)\n",
    "    base_df = base_df.merge(_stock_list, on='SecuritiesCode', how=\"left\")\n",
    "\n",
    "    # Combine with trades\n",
    "    # Edit the Section item of trades so that it is linked to the New Market Segment of stock_list.\n",
    "    # _trades = trades.copy()\n",
    "    # _trades['NewMarketSegment'] = _trades['Section'].str.split(' \\(', expand=True)[0]\n",
    "    # base_df = base_df.merge(_trades, on=['Date', 'NewMarketSegment'], how=\"left\")\n",
    "\n",
    "    # Combine with financials\n",
    "    # _financials = financials.copy()\n",
    "    # _financials.rename(columns={'Date': 'Date_x', 'SecuritiesCode': 'SecuritiesCode_x'}, inplace=True)\n",
    "    # base_df = base_df.merge(_financials, left_on='RowId', right_on='DateCode', how=\"left\")\n",
    "    \n",
    "    return base_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52x0CmnOTvJP"
   },
   "source": [
    "* The adjust_price function uses the function introduced in Train Demo almost as it is.(Only the indexing of Date is commented out)\n",
    "* It is beyond the scope of integration due to the addition of items, but it is being executed at this stage because operations such as sorting and index generation are performed within the function.\n",
    "* The function will generate an item called AdjustedClose.\n",
    "* Stock prices can fluctuate significantly due to splits and mergers, but by using Adjusted Close instead of Close, this effect can be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fCnjHsOZ4PCK"
   },
   "outputs": [],
   "source": [
    "def adjust_price(price):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        price (pd.DataFrame)  : pd.DataFrame include stock_price\n",
    "    Returns:\n",
    "        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n",
    "    \"\"\"\n",
    "    # transform Date column into datetime\n",
    "    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "    def generate_adjusted_close(df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n",
    "        Returns:\n",
    "            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n",
    "        \"\"\"\n",
    "        # sort data to generate CumulativeAdjustmentFactor\n",
    "        df = df.sort_values(\"Date\", ascending=False)\n",
    "        # generate CumulativeAdjustmentFactor\n",
    "        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n",
    "        # generate AdjustedClose\n",
    "        df.loc[:, \"AdjustedClose\"] = (\n",
    "            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n",
    "        ).map(lambda x: float(\n",
    "            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n",
    "        ))\n",
    "        # reverse order\n",
    "        df = df.sort_values(\"Date\")\n",
    "        # to fill AdjustedClose, replace 0 into np.nan\n",
    "        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n",
    "        # forward fill AdjustedClose\n",
    "        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n",
    "        return df\n",
    "\n",
    "    # generate AdjustedClose\n",
    "    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n",
    "    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n",
    "\n",
    "    # price.set_index(\"Date\", inplace=True)\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MqOILGX-4O_-"
   },
   "outputs": [],
   "source": [
    "def collector(prices, options, financials, trades, secondary_prices, stock_list):\n",
    "    # Consolidate the read data into one file\n",
    "    base_df = merge_data(prices, options, financials, trades, secondary_prices, stock_list)\n",
    "    # Generate AdjustedClose item\n",
    "    base_df = adjust_price(base_df)\n",
    "    \n",
    "    return base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JD6NonEF4O9j",
    "outputId": "f8dff3ff-571d-455e-d9be-5f620c39cb8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 30.4 s\n",
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_df = collector(*train_files, stock_list)\n",
    "supplemental_df = collector(*supplemental_files, stock_list)\n",
    "base_df = pd.concat([base_df, supplemental_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vwjF5FsWeFw"
   },
   "source": [
    "## Preprocessor (**FEATURE ENGINEERING**) \n",
    "* generate_features (add_columns_per_code =>  add columns_per_day) => select_features => **feature_df**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA6iROuCXs0A"
   },
   "source": [
    "* Select only those that generate features and contribute to improving the accuracy of inference results.\n",
    "* The calc_change_rate_base and calc_volatility_base functions are based on the functions introduced in the Train Demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fgZisooaZ_bR"
   },
   "outputs": [],
   "source": [
    "# A function that generates \"a function that derives the rate of change of the item \n",
    "# specified by column_name in the period (plural) specified by periods and adds it as an item\".\n",
    "# The generated function assumes that a dataframe with only a specific securities code is entered.\n",
    "def calc_change_rate_base(column_name, periods):\n",
    "    def func(price):\n",
    "        for period in periods:\n",
    "            price.loc[:, f\"{column_name}_change_rate_{period}\"] = price[column_name].pct_change(period)\n",
    "        return price\n",
    "    return func\n",
    "\n",
    "\n",
    "# A function that generates \"a function that derives the degree of fluctuation of the item \n",
    "# specified by column_name in the period (plural) specified by periods and adds it as an item\".\n",
    "#The generated function assumes that a dataframe with only a specific securities code is entered.\n",
    "def calc_volatility_base(column_name, periods):\n",
    "    def func(price):\n",
    "        for period in periods:\n",
    "            price.loc[:, f\"{column_name}_volatility_{period}\"] = np.log(price[column_name]).diff().rolling(window=period, min_periods=1).std()\n",
    "        return price\n",
    "    return func\n",
    "\n",
    "# A function that generates \"a function that derives the ratio of the moving average value and the current value of the item\n",
    "# specified by column_name in the period (plural) specified by periods and adds it as an item\".\n",
    "# The ratio to the current value, not the moving average value itself, is because the Target this time is a ratio.\n",
    "# The generated function assumes that a dataframe with only a specific securities code is entered.\n",
    "def calc_moving_average_rate_base(column_name, periods):\n",
    "    def func(price):\n",
    "        for period in periods:\n",
    "            price.loc[:, f\"{column_name}_average_rate_{period}\"] = price[column_name].rolling(window=period, min_periods=1).mean() / price[column_name]\n",
    "        return price\n",
    "    return func\n",
    "\n",
    "# A function that generates the volatility of the closing price and adds it as an item. \n",
    "# Shift-2 this to become Target.\n",
    "# This function assumes that a data frame with only a specific securities code is entered.\n",
    "def calc_target_shift2(price):\n",
    "    price.loc[:, 'Target_shift2'] = price['Close'].pct_change()\n",
    "    return price\n",
    "\n",
    "# A function that groups the input data frames by securities code and applies the function \n",
    "# passed as an argument.\n",
    "# Assuming that a list of calc_xxx functions defined in ↑ is passed to functions\n",
    "def add_columns_per_code(price, functions):\n",
    "    def func(df):\n",
    "        for f in functions:\n",
    "            df = f(df)\n",
    "        return df\n",
    "    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n",
    "    price = price.groupby(\"SecuritiesCode\").apply(func)\n",
    "    price = price.reset_index(drop=True)\n",
    "    return price\n",
    "\n",
    "# Functions that add features to the input data frame\n",
    "# Assuming that the item to be added basically uses only the value in the record\n",
    "def add_columns_per_day(base_df):\n",
    "    base_df['diff_rate1'] = (base_df['Close'] - base_df['Open']) / base_df['Close']\n",
    "    base_df['diff_rate2'] = (base_df['High'] - base_df['Low']) / base_df['Close']    \n",
    "    return base_df\n",
    "\n",
    "# Functions that add features to the input data frame\n",
    "def generate_features(base_df):\n",
    "    prev_column_names = base_df.columns\n",
    "    \n",
    "    periods = [3, 9]\n",
    "    functions = [\n",
    "        calc_change_rate_base(\"AdjustedClose\", periods), \n",
    "        calc_volatility_base(\"AdjustedClose\", periods), \n",
    "        calc_moving_average_rate_base(\"Volume\", periods), \n",
    "        calc_target_shift2\n",
    "    ]\n",
    "    \n",
    "    #Added feature amount for each securities code\n",
    "    # (feature amount that generates records for a certain period as input, such as moving average)\n",
    "    base_df = add_columns_per_code(base_df, functions)\n",
    "    # Added daily features (features that can be derived from the values ​​in the record)\n",
    "    base_df = add_columns_per_day(base_df)\n",
    "    \n",
    "    # Generate a list of added item names to make it easier to select features later\n",
    "    add_column_names = list(set(base_df.columns) - set(prev_column_names))\n",
    "    return base_df, add_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0r87eAyGaX4M"
   },
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "def select_features(feature_df, add_column_names, is_train):\n",
    "    \n",
    "    #Basic items\n",
    "    base_cols = ['RowId', 'Date', 'SecuritiesCode']\n",
    "    # Numerical features\n",
    "    numerical_cols = sorted(add_column_names)\n",
    "    # Category features\n",
    "    categorical_cols = ['NewMarketSegment', '33SectorCode', '17SectorCode']\n",
    "    # Objective variable\n",
    "    label_col = ['Target']\n",
    "    \n",
    "    # Feature value\n",
    "    feat_cols = numerical_cols + categorical_cols\n",
    "\n",
    "    # Filter items in the data frame to only the selected items\n",
    "    feature_df = feature_df[base_cols + feat_cols + label_col]\n",
    "    #Category items change dtype to category\n",
    "    feature_df[categorical_cols] = feature_df[categorical_cols].astype('category')\n",
    "\n",
    "    if is_train:\n",
    "        #For training data, delete the record with NA item\n",
    "        feature_df.dropna(inplace=True)\n",
    "    else:\n",
    "        # Complement NA items for inference data\n",
    "        feature_df[numerical_cols] = feature_df[numerical_cols].fillna(0)\n",
    "        feature_df[numerical_cols] = feature_df[numerical_cols].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    return feature_df, feat_cols, label_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KnALeZOIaZ-9"
   },
   "outputs": [],
   "source": [
    "def preprocessor(base_df, is_train=True):\n",
    "    feature_df = base_df.copy()\n",
    "    \n",
    "    ## Feature generation\n",
    "    feature_df, add_column_names = generate_features(feature_df)\n",
    "    \n",
    "    ## Feature selection\n",
    "    feature_df, feat_cols, label_col = select_features(feature_df, add_column_names, is_train)\n",
    "\n",
    "    return feature_df, feat_cols, label_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRX11LczaeCD",
    "outputId": "c7707f32-4075-4040-cd99-8aea9c96a6d6"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "feature_df, feat_cols, label_col = preprocessor(base_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmVB7Fv0dueK"
   },
   "source": [
    "## Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PlT-sswen-y"
   },
   "source": [
    "* Train and generate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewJSB4yAdyyL"
   },
   "outputs": [],
   "source": [
    "# A function that arranges predicted values ​​in descending order and assigns rank numbers.\n",
    "# In other words, a function that derives submission items from the objective variable\n",
    "def add_rank(df, col_name=\"pred\"):\n",
    "    df[\"Rank\"] = df.groupby(\"Date\")[col_name].rank(ascending=False, method=\"first\") - 1 \n",
    "    df[\"Rank\"] = df[\"Rank\"].astype(\"int\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8XQ2Jlheyao"
   },
   "source": [
    "* The calc_spread_return_sharpe function uses the function introduced in the Train Demo as it is.\n",
    "* If you pass the inferred Rank and the data frame containing the correct Target, it will calculate the score according to the evaluation method of the contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpI-YdOEe7Ew"
   },
   "outputs": [],
   "source": [
    "def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame): predicted results\n",
    "        portfolio_size (int): # of equities to buy/sell\n",
    "        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
    "    Returns:\n",
    "        (float): sharpe ratio\n",
    "    \"\"\"\n",
    "    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): predicted results\n",
    "            portfolio_size (int): # of equities to buy/sell\n",
    "            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n",
    "        Returns:\n",
    "            (float): spread return\n",
    "        \"\"\"\n",
    "        assert df['Rank'].min() == 0\n",
    "        assert df['Rank'].max() == len(df['Rank']) - 1\n",
    "        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n",
    "        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n",
    "        return purchase - short\n",
    "\n",
    "    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n",
    "    sharpe_ratio = buf.mean() / buf.std()\n",
    "    return sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcXxFQQ6e9B9"
   },
   "outputs": [],
   "source": [
    "# Data frame for prediction and a function to calculate the score based on the prediction result\n",
    "def evaluator(df, pred):\n",
    "    df[\"pred\"] = pred\n",
    "    df = add_rank(df)\n",
    "    score = calc_spread_return_sharpe(df)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ1Ah4kWfCt4"
   },
   "source": [
    "* By importing optuna.integration.lightgbm instead of lightgbm, piper parameter tuning will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kTcKZNQfEXY"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "# import optuna.integration.lightgbm as lgb\n",
    "\n",
    "# Functions that perform learning\n",
    "def trainer(feature_df, feat_cols, label_col, fold_params, seed=2022):\n",
    "    scores = []\n",
    "    models = []\n",
    "    params = []\n",
    "\n",
    "    for param in fold_params:\n",
    "        ################################\n",
    "        #Data preparation\n",
    "        ################################\n",
    "        train = feature_df[(param[0] <= feature_df['Date']) & (feature_df['Date'] < param[1])]\n",
    "        valid = feature_df[(param[1] <= feature_df['Date']) & (feature_df['Date'] < param[2])]\n",
    "\n",
    "        X_train = train[feat_cols]\n",
    "        y_train = train[label_col]\n",
    "        X_valid = valid[feat_cols]\n",
    "        y_valid = valid[label_col]\n",
    "        \n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "        ################################\n",
    "        # study\n",
    "        ################################\n",
    "        params = {\n",
    "            'task': 'train',                   \n",
    "            'boosting_type': 'gbdt',           \n",
    "            'objective': 'regression',         \n",
    "            'metric': 'rmse',                  \n",
    "            'learning_rate': 0.01,             \n",
    "            'lambda_l1': 0.5,                  \n",
    "            'lambda_l2': 0.5,                  \n",
    "            'num_leaves': 10,                  \n",
    "            'feature_fraction': 0.5,           \n",
    "            'bagging_fraction': 0.5,           \n",
    "            'bagging_freq': 5,                  \n",
    "            'min_child_samples': 10,           \n",
    "            'seed': seed                       \n",
    "        } \n",
    " \n",
    "        lgb_results = {}                       \n",
    "        model = lgb.train( \n",
    "            params,                            \n",
    "            lgb_train,                         \n",
    "            valid_sets=[lgb_train, lgb_valid], \n",
    "            valid_names=['Train', 'Valid'],    \n",
    "            num_boost_round=2000,              \n",
    "            early_stopping_rounds=100,         \n",
    "            evals_result=lgb_results,          \n",
    "            verbose_eval=100,                  \n",
    "        )  \n",
    "\n",
    "        ################################\n",
    "        # Result drawing\n",
    "        ################################\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "        # loss\n",
    "        plt.subplot(1,2,1)\n",
    "        loss_train = lgb_results['Train']['rmse']\n",
    "        loss_test = lgb_results['Valid']['rmse']   \n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('logloss')\n",
    "        plt.plot(loss_train, label='train loss')\n",
    "        plt.plot(loss_test, label='valid loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # feature importance\n",
    "        plt.subplot(1,2,2)\n",
    "        importance = pd.DataFrame({'feature':feat_cols, 'importance':model.feature_importance()})\n",
    "        sns.barplot(x = 'importance', y = 'feature', data = importance.sort_values('importance', ascending=False))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        ################################\n",
    "        # evaluation\n",
    "        ################################\n",
    "        # inference\n",
    "        pred =  model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "        # evaluation\n",
    "        score = evaluator(valid, pred)\n",
    "\n",
    "        scores.append(score)\n",
    "        models.append(model)\n",
    "\n",
    "    print(\"CV_SCORES:\", scores)\n",
    "    print(\"CV_SCORE:\", np.mean(scores))\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "o05MgmYzfLEu",
    "outputId": "59f47ad7-bede-43dd-9a9f-404f6c8e6697"
   },
   "outputs": [],
   "source": [
    "# Since the data before 2020-12-23 does not have all 2000 securities codes,\n",
    "# ,only the data after that will be used.\n",
    "# (Start date of training data, end date of training data = start date of verification data\n",
    "# , end date of verification data)\n",
    "fold_params = [\n",
    "    ('2020-12-23', '2021-11-01', '2021-12-01'),\n",
    "    ('2021-01-23', '2021-12-01', '2022-01-01'),\n",
    "    ('2021-02-23', '2022-01-01', '2022-02-01'),\n",
    "]\n",
    "models = trainer(feature_df, feat_cols, label_col, fold_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_3LLNXhfj4M"
   },
   "source": [
    "## Reasoning / evaluation\n",
    "* Use the generated model to infer test data and calculate the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxyDMs4mffHW"
   },
   "outputs": [],
   "source": [
    "def predictor(feature_df, feat_cols, models, is_train=True):\n",
    "    X = feature_df[feat_cols]\n",
    "    \n",
    "    # inference\n",
    "    preds = list(map(lambda model: model.predict(X, num_iteration=model.best_iteration), models))\n",
    "    \n",
    "    # Score is calculated only during learning\n",
    "    if is_train:\n",
    "        scores = list(map(lambda pred: evaluator(feature_df, pred), preds))\n",
    "        print(\"SCORES:\", scores)\n",
    "\n",
    "    # Bagging inference results\n",
    "    pred = np.array(preds).mean(axis=0)\n",
    "\n",
    "    # Score is calculated only during learning\n",
    "    if is_train:\n",
    "        score = evaluator(feature_df, pred)\n",
    "        print(\"SCORE:\", score)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2lp-BlpfEVC",
    "outputId": "09331859-5f0e-4e24-890f-4a7ae7eb9de0"
   },
   "outputs": [],
   "source": [
    "# Use test data that is not used for learning or verification\n",
    "test_df = feature_df[('2022-02-01' <= feature_df['Date'])].copy()\n",
    "pred = predictor(test_df, feat_cols, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oSlBdCihDWT"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVcB6aqNhLgJ"
   },
   "source": [
    "* Register the inference result using the time series API.\n",
    "* Since we have adopted a value that refers to past data such as moving averages as a feature quantity, it is necessary to implement a mechanism to store the data obtained from the time series API.\n",
    "* Regarding this mechanism, I referred to \"Start-to-finish demo based on s-meitoma + tweaks\".\n",
    "* https://www.kaggle.com/code/lowellniles/start-to-finish-demo-based-on-s-meitoma-tweaks\n",
    "* The code below is an implementation that stores historical data in a data frame called past_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHQhfeD03ev6"
   },
   "outputs": [],
   "source": [
    "#Load time series API\n",
    "import jpx_tokyo_market_prediction # we can not import jpx_tokyo_market_prediction out of kaggle for submissioon\n",
    "env = jpx_tokyo_market_prediction.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6jSSi_p3mFW"
   },
   "outputs": [],
   "source": [
    "# Set up supplemental files as the initial state of historical data\n",
    "past_df = supplemental_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XE8w4Ev23rOj"
   },
   "outputs": [],
   "source": [
    "# Daily reasoning / registration\n",
    "for i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(iter_test):\n",
    "    current_date = prices[\"Date\"].iloc[0]\n",
    "\n",
    "    if i == 0:\n",
    "        # To prevent leaks, delete future data from the data received from the time series API\n",
    "        past_df = past_df[past_df[\"Date\"] < current_date]\n",
    "\n",
    "    \n",
    "    #Delete old history to secure resources\n",
    "    threshold = (pd.Timestamp(current_date) - pd.offsets.BDay(80)).strftime(\"%Y-%m-%d\")\n",
    "    past_df = past_df[past_df[\"Date\"] >= threshold]\n",
    "    \n",
    "    #Integrate data received from time series API into historical data\n",
    "    base_df = collector(prices, options, financials, trades, secondary_prices, stock_list)\n",
    "    past_df = pd.concat([past_df, base_df]).reset_index(drop=True)\n",
    "\n",
    "    # Feature engineering\n",
    "    feature_df, feat_cols, label_col = preprocessor(past_df, False)\n",
    "\n",
    "    # Extract only predictable records\n",
    "    feature_df = feature_df[feature_df['Date'] == current_date]\n",
    "\n",
    "    \n",
    "    #Inference\n",
    "    feature_df[\"pred\"] = predictor(feature_df, feat_cols, models, False)\n",
    "\n",
    "    \n",
    "    # Derivate RANK from the inference result and reflect it in the submitted data\n",
    "    feature_df = add_rank(feature_df)\n",
    "    feature_map = feature_df.set_index('SecuritiesCode')['Rank'].to_dict()\n",
    "    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(feature_map)\n",
    "\n",
    "    \n",
    "    #Register result\n",
    "    env.predict(sample_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exzW074NfEOs"
   },
   "outputs": [],
   "source": [
    "stock_list \n",
    "train_files \n",
    "supplemental_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgwrxIbx_hzE"
   },
   "source": [
    "# End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5W6EGze_nKo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BRqoVtKOXaC"
   },
   "source": [
    "## adjclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oxzO9ohOWZv"
   },
   "outputs": [],
   "source": [
    "def adjust_price(price):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        price (pd.DataFrame)  : pd.DataFrame include stock_price\n",
    "    Returns:\n",
    "        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n",
    "    \"\"\"\n",
    "    # transform Date column into datetime\n",
    "    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "    def generate_adjusted_close(df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n",
    "        Returns:\n",
    "            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n",
    "        \"\"\"\n",
    "        # sort data to generate CumulativeAdjustmentFactor\n",
    "        df = df.sort_values(\"Date\", ascending=False)\n",
    "        # generate CumulativeAdjustmentFactor\n",
    "        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n",
    "        # generate AdjustedClose\n",
    "        df.loc[:, \"AdjustedClose\"] = (\n",
    "            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n",
    "        ).map(lambda x: float(\n",
    "            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n",
    "        ))\n",
    "        # reverse order\n",
    "        df = df.sort_values(\"Date\")\n",
    "        # to fill AdjustedClose, replace 0 into np.nan\n",
    "        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n",
    "        # forward fill AdjustedClose\n",
    "        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n",
    "        return df\n",
    "\n",
    "    # generate AdjustedClose\n",
    "    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n",
    "    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n",
    "\n",
    "    price.set_index(\"Date\", inplace=True)\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BflO6_oAOTZb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIFEYdu8Okze"
   },
   "source": [
    "## analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "9zhMUlOpXDZR",
    "outputId": "5928b4bd-2565-46c2-c6c5-a0348ac27b9b"
   },
   "outputs": [],
   "source": [
    "df_price = pd.concat([df_price, df_price_supplemental])\n",
    "# generate AdjustedClose\n",
    "df_price = adjust_price(df_price)\n",
    "price = df_price.loc[df_price[\"SecuritiesCode\"] == 7203].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "ILsnP3YNYkZG",
    "outputId": "cc7218bf-ab4b-4a2d-eb4f-5b38f37113de"
   },
   "outputs": [],
   "source": [
    "price = df_price.loc[df_price[\"SecuritiesCode\"] == 7203].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqnrpFl4YEUI"
   },
   "outputs": [],
   "source": [
    "price = price.filter(items=[\"SecuritiesCode\", 'Open','High','Low','Volume','AdjustedClose','Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usQ4XISqYkF8"
   },
   "outputs": [],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMQ9J8gEUSXA"
   },
   "outputs": [],
   "source": [
    "periods = [10, 21, 63]\n",
    "return_names = []\n",
    "for period in periods:\n",
    "    return_names.append(f\"return_{period}\")\n",
    "    price.loc[:, f\"return_{period}\"] = price[\"AdjustedClose\"].pct_change(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCl9GSKbXyJh"
   },
   "outputs": [],
   "source": [
    "price[return_names].plot(figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jav3eSssa1Uc"
   },
   "outputs": [],
   "source": [
    "# Returns\n",
    "def returns(df, n):\n",
    "    df[f\"returns_{n}\"] = df[\"AdjustedClose\"].pct_change(n)\n",
    "    return df\n",
    "price=returns(price, 10)\n",
    "price=returns(price, 21)\n",
    "price=returns(price, 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhUUMeMra3CF"
   },
   "outputs": [],
   "source": [
    "price.iloc[:,-3:].plot(figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fx7se7jfXZUu"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Volatilities\n",
    "def Volatilities(df, n):\n",
    "    df[f\"Volatilities_{n}\"] = np.log(df[\"AdjustedClose\"]).diff().rolling(n).std()\n",
    "    return df\n",
    "price=Volatilities(price, 10)\n",
    "price=Volatilities(price, 21)\n",
    "price=Volatilities(price, 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbtpzxLg0-9W"
   },
   "outputs": [],
   "source": [
    "price.iloc[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBv3izBAaKaV"
   },
   "outputs": [],
   "source": [
    "np.log(price[\"AdjustedClose\"]).diff().rolling(10).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoDK71LdYyWq"
   },
   "outputs": [],
   "source": [
    "np.log(price[\"AdjustedClose\"]).diff().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DV8fHdRoZXrp"
   },
   "outputs": [],
   "source": [
    "np.log(price[\"AdjustedClose\"]/price[\"AdjustedClose\"].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmFOfo_CaUHB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HWK9Lk9bWDE9",
    "FcdaViLoWOCK",
    "9vwjF5FsWeFw",
    "YmVB7Fv0dueK",
    "H_3LLNXhfj4M",
    "3oSlBdCihDWT",
    "IgwrxIbx_hzE",
    "2BRqoVtKOXaC"
   ],
   "name": "JPX LightGBM Demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
